{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "file = open('training-data-small.txt', 'r')\n",
    "#file = open('test.txt','r')\n",
    "\n",
    "label = [] # list for label\n",
    "no_of_data = [] # for creating the dictionary, gather all the x_data\n",
    "train_x = [] #\n",
    "for line in file:\n",
    "\tlx = line.split('\\t')\n",
    "\tly = lx[1].rstrip().split(',')\n",
    "\tlabel.append(lx[0]) # labels appending\n",
    "\ttrain_x.append(ly) # used for labeling\n",
    "\tno_of_data.extend(ly)\n",
    "\n",
    "max_len = max([len(i) for i in train_x])\n",
    "min_len = min([len(i) for i in train_x])\n",
    "mean_len = np.mean([len(i) for i in train_x])\n",
    "std_len = np.std([len(i) for i in train_x])\n",
    "\n",
    "print('max sentence length: ',max_len)\n",
    "print('min sentence length: ', min_len)\n",
    "print('mean:', mean_len)\n",
    "print('std:', std_len)\n",
    "\n",
    "# creating label array\n",
    "train_label = np.asarray(label).reshape(-1,)\n",
    "print('label array shape:', train_label.shape)\n",
    "\n",
    "train_dict = dict()\n",
    "\n",
    "for i,j in enumerate(no_of_data):\n",
    "\ttrain_dict[j] = i\n",
    "\n",
    "print('unique words:',len(train_dict.keys()))\n",
    "\n",
    "#print(train_dict)\n",
    "\n",
    "# making the dictionary start with zero\n",
    "for i,j in enumerate(train_dict.keys()):\n",
    "\ttrain_dict[j] = i\n",
    "\n",
    "#print(train_dict)\n",
    "\n",
    "file.close()\n",
    "\n",
    "# making the train_data as integers\n",
    "\n",
    "#for i in train_x:\n",
    "#\tfor j \n",
    "\n",
    "\n",
    "#train_data = np.asarray(train_x)\n",
    "#print(train_data)\n",
    "#print(train_data.shape)\n",
    "#print(train_data[0])\n",
    "\n",
    "\n",
    "train_data = np.asarray(train_x).reshape(-1,)\n",
    "for index,i in enumerate(train_data):\n",
    "\tfor pos,k in enumerate(i):\n",
    "\t\ttrain_data[index][pos] = train_dict[train_data[index][pos]]\n",
    "\t\t#print(ls_ara[index][j])\n",
    "\n",
    "print(train_data)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100\n",
    "X_train = sequence.pad_sequences(train_data, maxlen=max_words)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "top_words = 125\n",
    "max_words = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, train_label, validation_data=None, epochs=20, batch_size=32, verbose=2)\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_train[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train_1 = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "file = open('training-data-small.txt', 'r')\n",
    "#file = open('test.txt','r')\n",
    "\n",
    "label = [] # list for label\n",
    "no_of_data = [] # for creating the dictionary, gather all the x_data\n",
    "train_x = [] #\n",
    "for line in file:\n",
    "\tlx = line.split('\\t')\n",
    "\tly = lx[1].rstrip().split(',')\n",
    "\tlabel.append(lx[0]) # labels appending\n",
    "\ttrain_x.append(ly) # used for labeling\n",
    "\tno_of_data.extend(ly)\n",
    "\n",
    "max_len = max([len(i) for i in train_x])\n",
    "min_len = min([len(i) for i in train_x])\n",
    "mean_len = np.mean([len(i) for i in train_x])\n",
    "std_len = np.std([len(i) for i in train_x])\n",
    "\n",
    "print('max sentence length: ',max_len)\n",
    "print('min sentence length: ', min_len)\n",
    "print('mean:', mean_len)\n",
    "print('std:', std_len)\n",
    "\n",
    "# creating label array\n",
    "train_label = np.asarray(label).reshape(-1,)\n",
    "print('label array shape:', train_label.shape)\n",
    "\n",
    "train_dict = dict()\n",
    "\n",
    "for i,j in enumerate(no_of_data):\n",
    "\ttrain_dict[j] = i\n",
    "\n",
    "print('unique words:',len(train_dict.keys()))\n",
    "\n",
    "#print(train_dict)\n",
    "\n",
    "# making the dictionary start with zero\n",
    "for i,j in enumerate(train_dict.keys()):\n",
    "\ttrain_dict[j] = i\n",
    "\n",
    "#print(train_dict)\n",
    "\n",
    "file.close()\n",
    "\n",
    "# making the train_data as integers\n",
    "\n",
    "#for i in train_x:\n",
    "#\tfor j \n",
    "\n",
    "\n",
    "#train_data = np.asarray(train_x)\n",
    "#print(train_data)\n",
    "#print(train_data.shape)\n",
    "#print(train_data[0])\n",
    "\n",
    "\n",
    "train_data = np.asarray(train_x).reshape(-1,)\n",
    "for index,i in enumerate(train_data):\n",
    "\tfor pos,k in enumerate(i):\n",
    "\t\ttrain_data[index][pos] = train_dict[train_data[index][pos]]\n",
    "\t\t#print(ls_ara[index][j])\n",
    "\n",
    "print(train_data)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
