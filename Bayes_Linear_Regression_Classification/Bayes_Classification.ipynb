{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    x[x<0.0] = 0.0\n",
    "    return x\n",
    "\n",
    "def deriv_relu(x):\n",
    "    x[x>0.0] = 1.0\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return (1-x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Packages (load MNIST data and preprocess it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakur/newenv/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "# prepare data\n",
    "N = 70000\n",
    "data = np.float32(mnist.data[:]) / 255.\n",
    "idx = np.random.choice(data.shape[0], N)\n",
    "data = data[idx]\n",
    "target = np.int32(mnist.target[idx]).reshape(N, 1)\n",
    "\n",
    "train_idx, test_idx = train_test_split(np.array(range(N)), test_size=0.10)\n",
    "train_data, test_data = data[train_idx], data[test_idx]\n",
    "train_target, test_target = target[train_idx], target[test_idx]\n",
    "\n",
    "train_target = np.float32(preprocessing.OneHotEncoder(sparse=False).fit_transform(train_target))\n",
    "\n",
    "# inputs\n",
    "n_input = train_data.shape[1]\n",
    "M = train_data.shape[0]\n",
    "\n",
    "batch_size = 100 # 4\n",
    "num_batches = train_data.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the sigma prior, initializing the mu, sigma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_prior = np.exp(-3)\n",
    "learning_rate = 0.001 # from 0.001, changed from 0.0005 of linear regression\n",
    "n_epochs = 50\n",
    "n_hidden_1 = 400\n",
    "n_hidden_2 = 400\n",
    "n_output = 10\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(M / float(batch_size))\n",
    "\n",
    "mu_1 = np.random.normal(0, 0.05, size=(n_input, n_hidden_1))\n",
    "sigma_1 = np.random.normal(0, 0.05, size=(n_input, n_hidden_1))\n",
    "mu_2 = np.random.normal(0, 0.05, size=(n_hidden_1, n_hidden_2))\n",
    "sigma_2 = np.random.normal(0, 0.05, size=(n_hidden_1, n_hidden_2))\n",
    "mu_3 = np.random.normal(0, 0.05, size=(n_hidden_2, n_output))\n",
    "sigma_3 = np.random.normal(0, 0.05, size=(n_hidden_2, n_output))\n",
    "b_b1_mu = np.random.normal(0, 0.05, size=(1, n_hidden_1))\n",
    "b_b1_logsigma = np.random.normal(0, 0.05, size=(1, n_hidden_1))\n",
    "b_b2_mu = np.random.normal(0, 0.05, size=(1, n_hidden_2))\n",
    "b_b2_logsigma = np.random.normal(0, 0.05, size=(1, n_hidden_2))\n",
    "b_b3_mu = np.random.normal(0, 0.05, size=(1, n_output))\n",
    "b_b3_logsigma = np.random.normal(0, 0.05, size=(1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for forward and back propagtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backprop(mu_1, sigma_1, eps_1, mu_2, sigma_2, eps_2, mu_3, sigma_3, eps_3, \n",
    "    b_b1_mu, b_b1_logsigma, b_epsilon_b1,b_b2_mu, b_b2_logsigma, b_epsilon_b2,\n",
    "    b_b3_mu, b_b3_logsigma, b_epsilon_b3, x_in, y_output, batch_size, n_batches, lr, b, episode_num):\n",
    "    weight_1 = mu_1 + np.log(1. + np.exp(sigma_1))*eps_1\n",
    "    weight_2 = mu_2 + np.log(1. + np.exp(sigma_2))*eps_2\n",
    "    weight_3 = mu_3 + np.log(1. + np.exp(sigma_3))*eps_3\n",
    "    b_weight_1 = b_b1_mu + np.log(1+np.exp(b_b1_logsigma))*b_epsilon_b1\n",
    "    b_weight_2 = b_b2_mu + np.log(1+np.exp(b_b2_logsigma))*b_epsilon_b2\n",
    "    b_weight_3 = b_b3_mu + np.log(1+np.exp(b_b3_logsigma))*b_epsilon_b3\n",
    "    z1 = np.dot(x_in, weight_1) + b_weight_1\n",
    "    h1 = relu(z1)\n",
    "    z2 = np.dot(h1, weight_2) + b_weight_2 \n",
    "    h2 = relu(z2)\n",
    "    z3 = np.dot(h2, weight_3) + b_weight_3\n",
    "    z3 = np.exp(z3)/np.sum(np.exp(z3), axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    # likelihood\n",
    "    delta3 = -1.0*(1./x_in.shape[0])*(y_output-z3)/(np.exp(-3)**2) # we need to check (y-h) or (h-y)\n",
    "    delta_b3_mu = np.sum(delta3, axis=0)\n",
    "    delta_b3_logsigma = np.sum((delta3)*(1/(1+np.exp(-b_b3_logsigma)))*b_epsilon_b3, axis=0)\n",
    "    delta_w3_mu = np.dot(h2.T, delta3)\n",
    "    delta_w3_logsigma = np.dot(h2.T, delta3)*(1/(1+np.exp(-sigma_3)))*eps_3\n",
    "    delta2 = np.dot(delta3, weight_3.T)*deriv_relu(h2)\n",
    "    delta_b2_mu = np.sum(delta2, axis=0)\n",
    "    delta_b2_logsigma = np.sum((delta2)*(1/(1+np.exp(-b_b2_logsigma)))*b_epsilon_b2, axis=0)\n",
    "    delta_w2_mu = np.dot(h1.T, delta2)\n",
    "    delta_w2_logsigma = np.dot(h1.T, delta2)*(1/(1+np.exp(-sigma_2)))*eps_2\n",
    "    delta1 = np.dot(delta2, weight_2.T)*deriv_relu(h1)\n",
    "    delta_b1_mu = np.sum(delta1, axis=0)\n",
    "    delta_b1_logsigma = np.sum((delta1)*(1/(1+np.exp(-b_b1_logsigma)))*b_epsilon_b1, axis=0)\n",
    "    delta_w1_mu = np.dot(x_in.T, delta1)\n",
    "    delta_w1_logsigma = np.dot(x_in.T, delta1)*(1/(1+np.exp(-sigma_1)))*eps_1\n",
    "    \n",
    "    # prior \n",
    "    # wegihts\n",
    "    w_gaussian_prior_mu_1 = -weight_1/(0.05**2)\n",
    "    w_gaussian_prior_sigma_1 = (-weight_1/(0.05**2))* eps_1*(1./(1.+np.exp(-sigma_1)))\n",
    "    w_gaussian_prior_mu_2 = -weight_2/(0.05**2)\n",
    "    w_gaussian_prior_sigma_2 = (-weight_2/(0.05**2))* eps_2*(1./(1.+np.exp(-sigma_2)))\n",
    "    w_gaussian_prior_mu_3 = -weight_3/(0.05**2)\n",
    "    w_gaussian_prior_sigma_3 = (-weight_3/(0.05**2))* eps_3*(1./(1.+np.exp(-sigma_3)))\n",
    "    # biases\n",
    "    b_gaussian_prior_mu_1 = -b_weight_1/(0.05**2)\n",
    "    b_gaussian_prior_sigma_1 = (-b_weight_1/(0.05**2))* b_epsilon_b1*(1./(1.+np.exp(-b_b1_logsigma)))\n",
    "    b_gaussian_prior_mu_2 = -b_weight_2/(0.05**2)\n",
    "    b_gaussian_prior_sigma_2 = (-b_weight_2/(0.05**2))* b_epsilon_b2*(1./(1.+np.exp(-b_b2_logsigma)))\n",
    "    b_gaussian_prior_mu_3 = -b_weight_3/(0.05**2)\n",
    "    b_gaussian_prior_sigma_3 = (-b_weight_3/(0.05**2))* b_epsilon_b3*(1./(1.+np.exp(-b_b3_logsigma)))\n",
    "    \n",
    "    # variational posterior only sigmas\n",
    "    # sigmas for weights (mus for weights is zero in the case of Variational Posterior)\n",
    "    varitional_sigma_1 = (-1.0/(np.log(1. + np.exp(sigma_1))))*(1/(1+np.exp(-sigma_1)))\n",
    "    varitional_sigma_2 = (-1.0/(np.log(1. + np.exp(sigma_2))))*(1/(1+np.exp(-sigma_2)))\n",
    "    varitional_sigma_3 = (-1.0/(np.log(1. + np.exp(sigma_3))))*(1/(1+np.exp(-sigma_3)))\n",
    "    \n",
    "    # sigmas for bias terms\n",
    "    b_varitional_sigma_1 = (-1.0/(np.log(1+np.exp(b_b1_logsigma))))*(1/(1+np.exp(-b_b1_logsigma)))\n",
    "    b_varitional_sigma_2 = (-1.0/(np.log(1+np.exp(b_b2_logsigma))))*(1/(1+np.exp(-b_b2_logsigma)))\n",
    "    b_varitional_sigma_3 = (-1.0/(np.log(1+np.exp(b_b3_logsigma))))*(1/(1+np.exp(-b_b3_logsigma)))\n",
    "\n",
    "    # mus and sigmas updates\n",
    "    mu_1 = mu_1 - lr*((1./n_batches)*(0-w_gaussian_prior_mu_1)+(delta_w1_mu*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "    sigma_1 = sigma_1 - lr*((1./n_batches)*(varitional_sigma_1-w_gaussian_prior_sigma_1) + \\\n",
    "        (delta_w1_logsigma*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "\n",
    "    b_b1_mu = b_b1_mu - lr*((1./n_batches)*(0-b_gaussian_prior_mu_1)+(delta_b1_mu*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "    b_b1_logsigma = b_b1_logsigma - lr*((1./n_batches)*(b_varitional_sigma_1-b_gaussian_prior_sigma_1) + \\\n",
    "        (delta_b1_logsigma*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "\n",
    "    mu_2 = mu_2 - lr*((1./n_batches)*(0-w_gaussian_prior_mu_2)+ (delta_w2_mu*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "    sigma_2 = sigma_2 - lr*((1./n_batches)*(varitional_sigma_2-b_gaussian_prior_sigma_2) + \\\n",
    "        (delta_w2_logsigma*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "\n",
    "    b_b2_mu = b_b2_mu - lr*((1./n_batches)*(0-b_gaussian_prior_mu_2) + \\\n",
    "        (delta_b2_mu*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "    b_b2_logsigma = b_b2_logsigma - lr*((1./n_batches)*(b_varitional_sigma_2-b_gaussian_prior_sigma_2) + \\\n",
    "        (delta_b2_logsigma*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "\n",
    "    mu_3 = mu_3 - lr*((1./n_batches)*(0-w_gaussian_prior_mu_3)+(delta_w3_mu*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "    sigma_3 = sigma_3 - lr*((1./n_batches)*(varitional_sigma_3-w_gaussian_prior_sigma_3) + \\\n",
    "        (delta_w3_logsigma*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "\n",
    "    b_b3_mu = b_b3_mu - lr*((1./n_batches)*(0-b_gaussian_prior_mu_3)+(delta_b3_mu*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "    b_b3_logsigma = b_b3_logsigma - lr*((1./n_batches)*(b_varitional_sigma_3-b_gaussian_prior_sigma_3) + \\\n",
    "        (delta_b3_logsigma*x_in.shape[0]))*(1./x_in.shape[0])\n",
    "\n",
    "    # this method returns the new mus and sigmas after updating\n",
    "    return (mu_1, sigma_1, mu_2, sigma_2, mu_3, sigma_3, b_b1_mu, b_b1_logsigma, b_b2_mu, b_b2_logsigma, b_b3_mu, b_b3_logsigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 test accuracy 0.9592857142857143 train accuracy 0.9666190476190476\n",
      "epoch 1 test accuracy 0.9737142857142858 train accuracy 0.9811587301587301\n",
      "epoch 2 test accuracy 0.9805714285714285 train accuracy 0.9873650793650793\n",
      "epoch 3 test accuracy 0.9832857142857143 train accuracy 0.9906507936507937\n",
      "epoch 4 test accuracy 0.9862857142857143 train accuracy 0.9935079365079366\n",
      "epoch 5 test accuracy 0.9868571428571429 train accuracy 0.9943333333333333\n",
      "epoch 6 test accuracy 0.9887142857142858 train accuracy 0.997\n",
      "epoch 7 test accuracy 0.9897142857142858 train accuracy 0.9974285714285714\n",
      "epoch 8 test accuracy 0.9887142857142858 train accuracy 0.9969365079365079\n",
      "epoch 9 test accuracy 0.9907142857142858 train accuracy 0.9987619047619047\n",
      "epoch 10 test accuracy 0.9912857142857143 train accuracy 0.9986825396825397\n",
      "epoch 11 test accuracy 0.9907142857142858 train accuracy 0.9991746031746032\n",
      "epoch 12 test accuracy 0.9918571428571429 train accuracy 0.9991111111111111\n",
      "epoch 13 test accuracy 0.993 train accuracy 0.9996190476190476\n",
      "epoch 14 test accuracy 0.9915714285714285 train accuracy 0.9992063492063492\n",
      "epoch 15 test accuracy 0.9927142857142857 train accuracy 0.9995873015873016\n",
      "epoch 16 test accuracy 0.9934285714285714 train accuracy 0.998968253968254\n",
      "epoch 17 test accuracy 0.9934285714285714 train accuracy 0.9994444444444445\n",
      "epoch 18 test accuracy 0.9927142857142857 train accuracy 0.9998095238095238\n",
      "epoch 19 test accuracy 0.9932857142857143 train accuracy 0.9995714285714286\n",
      "epoch 20 test accuracy 0.9935714285714285 train accuracy 0.999968253968254\n",
      "epoch 21 test accuracy 0.9937142857142857 train accuracy 0.9998412698412699\n",
      "epoch 22 test accuracy 0.9931428571428571 train accuracy 0.9999206349206349\n",
      "epoch 23 test accuracy 0.9934285714285714 train accuracy 0.9996666666666667\n",
      "epoch 24 test accuracy 0.9941428571428571 train accuracy 0.9997936507936508\n",
      "epoch 25 test accuracy 0.9934285714285714 train accuracy 0.9999047619047619\n",
      "epoch 26 test accuracy 0.9937142857142857 train accuracy 0.9999365079365079\n",
      "epoch 27 test accuracy 0.994 train accuracy 0.999968253968254\n",
      "epoch 28 test accuracy 0.9932857142857143 train accuracy 0.9999047619047619\n",
      "epoch 29 test accuracy 0.9934285714285714 train accuracy 0.9999523809523809\n",
      "epoch 30 test accuracy 0.9937142857142857 train accuracy 0.9994920634920635\n",
      "epoch 31 test accuracy 0.9937142857142857 train accuracy 1.0\n",
      "epoch 32 test accuracy 0.9937142857142857 train accuracy 0.9999365079365079\n",
      "epoch 33 test accuracy 0.994 train accuracy 1.0\n",
      "epoch 34 test accuracy 0.9937142857142857 train accuracy 0.999984126984127\n",
      "epoch 35 test accuracy 0.9934285714285714 train accuracy 0.9998571428571429\n",
      "epoch 36 test accuracy 0.9938571428571429 train accuracy 0.999968253968254\n",
      "epoch 37 test accuracy 0.9934285714285714 train accuracy 0.999984126984127\n",
      "epoch 38 test accuracy 0.9928571428571429 train accuracy 0.9998888888888889\n",
      "epoch 39 test accuracy 0.9935714285714285 train accuracy 0.9999206349206349\n",
      "epoch 40 test accuracy 0.993 train accuracy 1.0\n",
      "epoch 41 test accuracy 0.9934285714285714 train accuracy 1.0\n",
      "epoch 42 test accuracy 0.9941428571428571 train accuracy 0.9999365079365079\n",
      "epoch 43 test accuracy 0.9935714285714285 train accuracy 0.999984126984127\n",
      "epoch 44 test accuracy 0.994 train accuracy 1.0\n",
      "epoch 45 test accuracy 0.9938571428571429 train accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "    test_accuracy = []\n",
    "    train_accuracy = []\n",
    "\n",
    "    for b in range(n_batches):\n",
    "        x_input = np.reshape(train_data[b*batch_size:(b+1)*batch_size,:], (batch_size,784))\n",
    "        y_true = np.reshape(train_target[b*batch_size:(b+1)*batch_size,:],(batch_size,10))\n",
    "        eps_1 = np.random.normal(0, 0.05, size=(n_input, n_hidden_1))\n",
    "        eps_2 = np.random.normal(0, 0.05, size=(n_hidden_1, n_hidden_2))\n",
    "        eps_3 = np.random.normal(0, 0.05, size=(n_hidden_2, n_output))\n",
    "        b_epsilon_b1 = np.random.normal(0, 0.05, size=(1, n_hidden_1))\n",
    "        b_epsilon_b2 = np.random.normal(0, 0.05, size=(1, n_hidden_2))\n",
    "        b_epsilon_b3 = np.random.normal(0, 0.05, size=(1, n_output))\n",
    "        mu_1, sigma_1, mu_2, sigma_2, mu_3, sigma_3, b_b1_mu, b_b1_logsigma, b_b2_mu, b_b2_logsigma, b_b3_mu, \\\n",
    "        b_b3_logsigma = forward_backprop(mu_1, sigma_1, eps_1, mu_2, sigma_2, eps_2, mu_3, sigma_3, eps_3, b_b1_mu, \\\n",
    "                               b_b1_logsigma, b_epsilon_b1, b_b2_mu, b_b2_logsigma, b_epsilon_b2, b_b3_mu, b_b3_logsigma, b_epsilon_b3, x_input, y_true, batch_size, n_batches, learning_rate, b, e)\n",
    "\n",
    "    if e%1 == 0.0:\n",
    "        z1 = np.dot(train_data, mu_1) + b_b1_mu\n",
    "        h1 = relu(z1)\n",
    "        z2 = np.dot(h1, mu_2) + b_b2_mu \n",
    "        h2 = relu(z2)\n",
    "        z3 = np.dot(h2, mu_3) + b_b3_mu\n",
    "        out_1 = np.exp(z3)/np.sum(np.exp(z3), axis=1, keepdims=True)      \n",
    "\n",
    "        soft_arg_max_1 = np.asarray(np.argmax(out_1, axis=1))\n",
    "        test_numeric_1 = np.argmax(train_target, axis=1)\n",
    "        acc_1 = np.count_nonzero(soft_arg_max_1 == test_numeric_1)\n",
    "        #print('episode num', e)\n",
    "        train_accuracy.append(acc_1/soft_arg_max_1.shape[0])\n",
    "        \n",
    "        z1 = np.dot(test_data, mu_1) + b_b1_mu\n",
    "        h1 = relu(z1)\n",
    "        z2 = np.dot(h1, mu_2) + b_b2_mu \n",
    "        h2 = relu(z2)\n",
    "        z3 = np.dot(h2, mu_3) + b_b3_mu\n",
    "        out_2 = np.exp(z3)/np.sum(np.exp(z3), axis=1, keepdims=True)      \n",
    "\n",
    "        soft_arg_max_2 = np.asarray(np.argmax(out_2, axis=1))\n",
    "        #test_numeric_2 = np.argmax(test_target, axis=1)\n",
    "        count = 0\n",
    "\n",
    "        for i in range(len(test_target)):\n",
    "            if test_target[i] == soft_arg_max_2[i]:\n",
    "                count += 1\n",
    "        print(\"epoch\", e,\"test accuracy\",(count/len(test_target)),\"train accuracy\",(acc_1/soft_arg_max_1.shape[0]))\n",
    "        test_accuracy.append(count/len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
